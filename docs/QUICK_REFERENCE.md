# Quick Reference: Motion Tracking Training Pipeline

## Three-Stage Training Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 1: TRAIN (8B frames, ~15 hours on 4Ã—A100)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Purpose: Train teacher policy with privileged information              â”‚
â”‚                                                                         â”‚
â”‚ Rollout:  [encoder_priv â†’ actor_teacher] + privileged observations     â”‚
â”‚                                                                         â”‚
â”‚ Training:                                                               â”‚
â”‚   âœ“ PPO updates: actor_teacher + critic                                â”‚
â”‚   âœ“ MSE loss: adapt_module (learns to predict priv features)           â”‚
â”‚                                                                         â”‚
â”‚ Config: +exp=train, vecnorm=train, lr=5e-4                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
                        Checkpoint: teacher weights +
                                adapt_module (partially trained)
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 2: ADAPT (1B frames, ~2 hours on 4Ã—A100)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Purpose: Train adaptation module to predict privileged features        â”‚
â”‚                                                                         â”‚
â”‚ Rollout:  [adapt_module â†’ actor_student] + policy observations only    â”‚
â”‚           (student uses Stage 1 weights, frozen)                       â”‚
â”‚                                                                         â”‚
â”‚ Training:                                                               â”‚
â”‚   âœ“ MSE loss ONLY: adapt_module predicts priv_features                 â”‚
â”‚   âœ— NO PPO updates                                                      â”‚
â”‚   âœ— actor_student FROZEN (no weight updates)                           â”‚
â”‚   âœ— critic FROZEN (no weight updates)                                  â”‚
â”‚                                                                         â”‚
â”‚ Key Point: This is SUPERVISED LEARNING, not RL!                        â”‚
â”‚            Trains estimator with 2 mini-epochs per rollout             â”‚
â”‚                                                                         â”‚
â”‚ Config: +exp=adapt, vecnorm=eval, lr=5e-4, train_every=16              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
                        Checkpoint: adapt_module (trained) +
                                student (still from Stage 1)
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 3: FINETUNE (4B frames, ~8 hours on 4Ã—A100)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Purpose: Finetune student policy with frozen adaptation module         â”‚
â”‚                                                                         â”‚
â”‚ Rollout:  [adapt_module (FROZEN) â†’ actor_student]                      â”‚
â”‚                                                                         â”‚
â”‚ Training:                                                               â”‚
â”‚   Phase A (first 2.5% of training):                                    â”‚
â”‚     âœ“ PPO updates: critic ONLY                                         â”‚
â”‚     âœ— actor_student FROZEN                                             â”‚
â”‚                                                                         â”‚
â”‚   Phase B (remaining 97.5%):                                           â”‚
â”‚     âœ“ PPO updates: actor_student + critic                              â”‚
â”‚                                                                         â”‚
â”‚ Config: +exp=finetune, vecnorm=eval, lr=1e-4, train_every=16           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
                        Final Model: deploy-ready student policy
```

## Key Network States by Stage

| Network         | Stage 1 (TRAIN) | Stage 2 (ADAPT) | Stage 3 (FINETUNE) |
|-----------------|-----------------|-----------------|---------------------|
| encoder_priv    | âœ… Trained      | ğŸ”’ Not used     | ğŸ”’ Not used        |
| actor_teacher   | âœ… Trained      | ğŸ”’ Not used     | ğŸ”’ Not used        |
| actor_student   | ğŸ”’ Not trained  | ğŸ”’ Frozen       | âœ… Trained         |
| critic          | âœ… Trained      | ğŸ”’ Frozen       | âœ… Trained         |
| adapt_module    | âœ… Trained      | âœ… Trained      | ğŸ”’ Frozen          |

## Data Flow Per Stage

### Stage 1: TRAIN
```
Policy Obs â†’ encoder_priv â†’ priv_features â†’ actor_teacher â†’ actions
Priv Obs  â†—                                                      â†“
                                                            Environment
Critic Obs â†’ critic â†’ value estimate                            â†“
                                                              Next State
                                                                 â†“
Policy Obs â†’ adapt_module â†’ priv_pred â”€â”                   Compute Rewards
True Priv  â†’ encoder_priv â†’ priv_true â”€â”´â”€â†’ MSE Loss           â†“
                                            â†“               PPO Update
                                         Update              (Teacher)
```

### Stage 2: ADAPT
```
Policy Obs â†’ adapt_module â†’ priv_pred â†’ actor_student â†’ actions
                                 â†“                           â†“
True Priv  â†’ encoder_priv â†’ priv_true              Environment (collect data)
                                 â†“                           â†“
                            MSE Loss                     No RL Training!
                                 â†“                    (Just data collection)
                         Update adapt_module
```

### Stage 3: FINETUNE
```
Policy Obs â†’ adapt_module â†’ priv_pred â†’ actor_student â†’ actions
           (FROZEN)                    (UNFROZEN)          â†“
                                                      Environment
Critic Obs â†’ critic â†’ value estimate                      â†“
          (UNFROZEN)                                  Compute Rewards
                                                           â†“
                                                      PPO Update
                                                   (Student + Critic)
```

## Command Examples

### Run Full Pipeline
```bash
bash train.sh
```

### Run Individual Stages
```bash
# Stage 1: Train
uv run torchrun --nproc_per_node=4 scripts/train.py \
  task=G1/G1_tracking +exp=train \
  wandb.id=my_train_run

# Stage 2: Adapt
uv run torchrun --nproc_per_node=4 scripts/train.py \
  task=G1/G1_tracking +exp=adapt \
  checkpoint_path=run:PROJECT/my_train_run \
  wandb.id=my_adapt_run

# Stage 3: Finetune
uv run torchrun --nproc_per_node=4 scripts/train.py \
  task=G1/G1_tracking +exp=finetune \
  checkpoint_path=run:PROJECT/my_adapt_run \
  wandb.id=my_finetune_run
```

### Evaluate Policy
```bash
# Play in simulation
uv run scripts/eval.py --run_path PROJECT/my_finetune_run -p

# Export to ONNX for deployment
uv run scripts/eval.py --run_path PROJECT/my_finetune_run -p --export
```

## Key Configuration Parameters

| Parameter       | Train   | Adapt   | Finetune |
|-----------------|---------|---------|----------|
| total_frames    | 8B      | 1B      | 4B       |
| train_every     | 32      | 16      | 16       |
| lr              | 5e-4    | 5e-4    | 1e-4     |
| entropy_coef    | 0.01â†’0.0025 | 0.005â†’0.002 | 0.0025â†’0.0005 |
| vecnorm         | train   | eval    | eval     |
| ppo_epochs      | 5       | -       | 5        |
| num_minibatches | 8       | 8       | 8        |

## Important Notes

1. **Stage 2 (ADAPT) is NOT reinforcement learning** - it's supervised learning to train the estimator. The policy networks remain frozen.

2. **The student actor is never directly trained during ADAPT** - it only gets updated in FINETUNE stage.

3. **Privileged information is only available during training** - at deployment, only the student policy with adaptation module is used.

4. **The 2.5% critic warmup in FINETUNE** helps stabilize value estimates before updating the actor.

5. **Training order matters** - each stage builds on the previous checkpoint, so they must be run sequentially.

## File Structure Quick Reference

```
active_adaptation/
â”œâ”€â”€ envs/
â”‚   â”œâ”€â”€ locomotion.py          # SimpleEnv: main environment
â”‚   â”œâ”€â”€ mdp/
â”‚   â”‚   â”œâ”€â”€ observations.py    # Observation groups (policy, priv, critic_priv)
â”‚   â”‚   â”œâ”€â”€ rewards/           # Reward functions
â”‚   â”‚   â”œâ”€â”€ terminations.py    # Episode termination
â”‚   â”‚   â””â”€â”€ randomizations.py  # Domain randomization
â”‚   â””â”€â”€ scene.py               # MuJoCo scene setup
â”œâ”€â”€ learning/
â”‚   â”œâ”€â”€ ppo/
â”‚   â”‚   â””â”€â”€ ppo.py            # PPOPolicy: main RL algorithm
â”‚   â””â”€â”€ modules/              # Neural network modules
â””â”€â”€ utils/
    â”œâ”€â”€ motion.py             # Motion dataset loading
    â””â”€â”€ symmetry.py           # Left-right symmetry transforms

scripts/
â”œâ”€â”€ train.py                  # Main training script
â””â”€â”€ eval.py                   # Evaluation script

cfg/
â”œâ”€â”€ train.yaml               # Base training config
â”œâ”€â”€ exp/
â”‚   â”œâ”€â”€ train.yaml          # Stage 1 config
â”‚   â”œâ”€â”€ adapt.yaml          # Stage 2 config
â”‚   â””â”€â”€ finetune.yaml       # Stage 3 config
â””â”€â”€ task/
    â””â”€â”€ G1/                 # G1 robot configs
```

## Troubleshooting

**Q: Training is slow / OOM errors**
- Reduce `num_envs` in `cfg/task/G1/G1.yaml`
- Reduce `NPROC` in `train.sh`
- May increase training time

**Q: Adapt stage seems to do nothing**
- This is expected! ADAPT only trains the estimator, not the policy
- Check WandB for `adapt/estimator_loss` - it should decrease
- Policy performance will improve in FINETUNE stage

**Q: Student performs worse than teacher**
- Increase ADAPT training frames
- Increase FINETUNE training frames
- Check that adaptation module loss converged in Stage 2

**Q: How to resume training?**
- Set `checkpoint_path` to load from W&B run
- Can resume any stage from its checkpoint

## Performance Expectations

**Training Time** (4Ã—A100 GPUs):
- Stage 1 (TRAIN): ~15 hours
- Stage 2 (ADAPT): ~2 hours
- Stage 3 (FINETUNE): ~8 hours
- **Total**: ~25 hours

**GPU Memory**:
- ~30-40GB per GPU with 4096 envs
- Can be reduced by lowering `num_envs`

**Final Performance**:
- Student should achieve 85-95% of teacher performance
- Real robot deployment uses student policy only
